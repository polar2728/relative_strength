import streamlit as st
import pandas as pd
import numpy as np
import requests
import io
import warnings
import time
import threading
from datetime import datetime, timedelta
from kiteconnect import KiteConnect
from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_exception_type
from concurrent.futures import ThreadPoolExecutor, as_completed

warnings.filterwarnings("ignore")
st.set_page_config(page_title="NSE RS Leaders Scanner PRO", layout="wide")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIG
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RS_LOOKBACK_6M = 126
RS_LOOKBACK_3M = 63
MIN_RS_RANK = 80
MIN_LIQUIDITY_CR = 5

BENCHMARK_CANDIDATES = {
    "Nifty 50": "NIFTY 50",
    "Nifty Next 50": "NIFTY NEXT 50",
    "Nifty 100": "NIFTY 100",
    "Nifty 200": "NIFTY 200",
    "Nifty 500": "NIFTY 500",
    "Nifty Midcap 150": "NIFTY MIDCAP 150",
    "Nifty Total Market": "NIFTY TOTAL MARKET",
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RATE LIMITER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class RateLimiter:
    """Thread-safe rate limiter for Kite API (3 requests/second)"""
    def __init__(self, max_calls=3, period=1.0):
        self.max_calls = max_calls
        self.period = period
        self.calls = []
        self.lock = threading.Lock()
    
    def __call__(self, func):
        def wrapper(*args, **kwargs):
            with self.lock:
                now = time.time()
                # Remove calls older than period
                self.calls = [call for call in self.calls if call > now - self.period]
                
                if len(self.calls) >= self.max_calls:
                    sleep_time = self.period - (now - self.calls[0])
                    if sleep_time > 0:
                        time.sleep(sleep_time)
                
                self.calls.append(time.time())
            
            return func(*args, **kwargs)
        return wrapper

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLOUD SECRETS AUTH - NO USER INPUT REQUIRED
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_kite_from_secrets():
    """Get pre-configured Kite from Streamlit secrets"""
    try:
        # Access secrets set in Streamlit Cloud dashboard
        api_key = st.secrets["KITE_API_KEY"]
        access_token = st.secrets["KITE_ACCESS_TOKEN"]  # Your 24h token
        
        kite = KiteConnect(api_key=api_key)
        kite.set_access_token(access_token)
        
        # Quick validation
        profile = kite.profile()
        
        st.sidebar.success(f"âœ… Connected: {profile['user_name']}")
        return kite
        
    except Exception as e:
        st.sidebar.error(f"âŒ Auth failed: Update secrets â†’ {str(e)[:50]}")
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# KITE AUTH (FALLBACK OPTION)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def kite_auth_section():
    st.sidebar.markdown("### ğŸ”Œ Kite Connect")

    api_key = st.sidebar.text_input(
        "API Key", type="password",
        value=st.session_state.get("api_key", "")
    )
    access_token = st.sidebar.text_input(
        "Access Token", type="password",
        value=st.session_state.get("access_token", "")
    )

    if st.sidebar.button("TEST CONNECTION", use_container_width=True):
        if not api_key or not access_token:
            st.sidebar.error("Enter API key + token")
            return None

        try:
            kite = KiteConnect(api_key=api_key)
            kite.set_access_token(access_token)
            profile = kite.profile()

            st.session_state.api_key = api_key
            st.session_state.access_token = access_token
            st.session_state.kite = kite

            st.sidebar.success(f"Connected: {profile['user_name']}")
        except Exception as e:
            st.sidebar.error(str(e))

    kite = st.session_state.get("kite")
    if kite:
        st.sidebar.success("Kite ready")
    else:
        st.sidebar.warning("Not connected")

    return kite

def get_kite():
    if "kite" in st.session_state:
        return st.session_state.kite

    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# INSTRUMENT MAP - SEPARATE EQUITY & INDEX MAPS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(ttl=86400 * 7)
def load_kite_instrument_map(_kite):
    """Load instrument tokens with separate maps for equities and indices"""
    instruments = _kite.instruments("NSE")
    df = pd.DataFrame(instruments)

    eq = df[(df["segment"] == "NSE") & (df["exchange"] == "NSE")]
    idx = df[df["segment"] == "INDICES"]

    eq_map = dict(zip(eq["tradingsymbol"], eq["instrument_token"]))
    idx_map = dict(zip(idx["tradingsymbol"], idx["instrument_token"]))

    st.sidebar.success(f"ğŸ“Š {len(eq_map)} equities, {len(idx_map)} indices loaded")
    
    return {"equities": eq_map, "indices": idx_map}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PRE-FILTER USING QUOTE API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def prefilter_universe(kite, symbols, instrument_map, min_liq_cr=5):
    """Pre-filter universe using quote API before historical fetch"""
    equity_map = instrument_map["equities"]
    tokens = []
    symbol_map = {}
    
    for sym in symbols:
        clean = sym.replace(".NS", "")
        token = equity_map.get(clean)
        if token:
            key = f"NSE:{clean}"
            tokens.append(key)
            symbol_map[key] = sym
    
    if not tokens:
        st.warning("âš ï¸ No valid instrument tokens found for symbols")
        return symbols  # Return all symbols if quote API fails
    
    # Fetch quotes in batches of 500
    passed = []
    total_batches = (len(tokens) + 499) // 500
    
    for batch_idx in range(0, len(tokens), 500):
        batch = tokens[batch_idx:batch_idx+500]
        batch_num = (batch_idx // 500) + 1
        
        try:
            quotes = kite.quote(batch)
            
            for instrument, data in quotes.items():
                ltp = data.get('last_price', 0)
                volume = data.get('volume', 0)
                
                if ltp > 0 and volume > 0:
                    # Rough daily liquidity estimate (current price * volume)
                    daily_liq = (ltp * volume) / 1e7  # in crores
                    
                    # Pass if >= 30% of min threshold (very conservative for pre-filter)
                    if daily_liq >= min_liq_cr * 0.3:
                        passed.append(symbol_map[instrument])
            
            # Rate limit: ~0.35s between batches
            if batch_num < total_batches:
                time.sleep(0.35)
                
        except Exception as e:
            st.warning(f"Quote API error for batch {batch_num}: {e}")
            # On error, pass all symbols from this batch to be safe
            for inst in batch:
                if inst in symbol_map:
                    passed.append(symbol_map[inst])
    
    # If pre-filter failed completely, return all symbols
    if len(passed) == 0:
        st.warning("âš ï¸ Pre-filter failed, processing all symbols")
        return symbols
    
    return passed

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HISTORICAL DATA - OPTIMIZED WITH SHORTER DATE RANGE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@retry(
    stop=stop_after_attempt(3),
    wait=wait_fixed(2),
    retry=retry_if_exception_type((requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError))
)
def fetch_kite_historical_core(kite, symbol, days=300, instrument_map=None):
    """Core fetch function - reduced to 300 days (enough for 200 DMA + lookback)"""
    from_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
    to_date   = datetime.now().strftime("%Y-%m-%d")

    is_index = symbol in BENCHMARK_CANDIDATES.values()
    key = symbol if is_index else symbol.replace(".NS", "")

    # Use appropriate map based on symbol type
    token = instrument_map["indices"].get(key) if is_index else instrument_map["equities"].get(key)

    if token is None:
        print(f"â†’ Skipping {symbol} | No instrument_token found for key '{key}'")
        print(f"   Available index keys: {list(instrument_map['indices'].keys())[:10]}")
        return pd.DataFrame()

    try:
        token = int(token)
    except (ValueError, TypeError):
        print(f"â†’ Invalid token type for {symbol}: {token}")
        return pd.DataFrame()

    print(f"â†’ Fetching {symbol} | token={token} | {from_date} â†’ {to_date}")

    try:
        data = kite.historical_data(
            instrument_token=token,
            from_date=from_date,
            to_date=to_date,
            interval="day",
            continuous=False,
            oi=False
        )

        if not data:
            print(f"â†’ No data returned for {symbol}")
            return pd.DataFrame()

        df = pd.DataFrame(data)
        df["date"] = pd.to_datetime(df["date"])
        df.set_index("date", inplace=True)
        df = df[["open", "high", "low", "close", "volume"]]
        df.columns = ["Open", "High", "Low", "Close", "Volume"]
        print(f"â† {symbol}: {len(df)} rows fetched")
        return df

    except Exception as e:
        print(f"!!! FAILED {symbol}: {type(e).__name__} â†’ {str(e)}")
        return pd.DataFrame()

# Rate-limited wrapper
rate_limiter = RateLimiter(max_calls=3, period=1.0)

@rate_limiter
def fetch_kite_historical(kite, symbol, days=300, instrument_map=None):
    """Rate-limited wrapper for historical data fetch"""
    return fetch_kite_historical_core(kite, symbol, days, instrument_map)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PARALLEL BATCH FETCH
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_in_batches_parallel(kite, symbols, instrument_map, max_workers=5):
    """Parallel fetch with rate limiting - respects 3 req/sec limit"""
    all_data = {}
    total = len(symbols)
    prog = st.sidebar.progress(0.0)
    status = st.sidebar.empty()
    
    processed = 0
    successful = 0
    lock = threading.Lock()
    last_update = time.time()
    
    def fetch_symbol(sym):
        nonlocal processed, successful, last_update
        try:
            df = fetch_kite_historical(kite, sym, days=300, instrument_map=instrument_map)
            
            with lock:
                all_data[sym] = df
                if not df.empty:
                    successful += 1
                processed += 1
                
                # Update UI every 2 seconds
                now = time.time()
                if now - last_update > 2:
                    prog.progress(processed / total)
                    status.info(f"â³ {processed}/{total} | {successful} valid")
                    last_update = now
            
            return sym, df
        except Exception as e:
            with lock:
                all_data[sym] = pd.DataFrame()
                processed += 1
            print(f"Error fetching {sym}: {e}")
            return sym, pd.DataFrame()
    
    status.info(f"ğŸš€ Starting parallel fetch with {max_workers} workers...")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(fetch_symbol, sym): sym for sym in symbols}
        
        for future in as_completed(futures):
            try:
                future.result()
            except Exception as e:
                print(f"Future error: {e}")
    
    status.success(f"âœ… Complete: {successful}/{total} symbols fetched")
    prog.progress(1.0)
    return all_data

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SEQUENTIAL FETCH FOR BENCHMARKS (MORE RELIABLE)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_benchmarks_sequential(kite, instrument_map):
    """Fetch benchmarks sequentially with better error handling"""
    bm_data = {}
    
    for name, sym in BENCHMARK_CANDIDATES.items():
        st.sidebar.info(f"Fetching {name}...")
        df = fetch_kite_historical(kite, sym, days=300, instrument_map=instrument_map)
        
        if not df.empty:
            bm_data[sym] = df
            st.sidebar.success(f"âœ“ {name}: {len(df)} days")
        else:
            st.sidebar.warning(f"âœ— {name}: Failed")
        
        time.sleep(0.35)  # Rate limit
    
    return bm_data

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UNIVERSE LOADERS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(ttl=86400)
def load_nse_universe():
    """Load full NSE equity universe"""
    url = "https://archives.nseindia.com/content/equities/EQUITY_L.csv"
    df = pd.read_csv(url)
    df.columns = df.columns.str.upper().str.strip()

    df = df[df["SERIES"] == "EQ"]
    df["Symbol"] = df["SYMBOL"] + ".NS"
    name_map = dict(zip(df["SYMBOL"], df["NAME OF COMPANY"]))
    return df["Symbol"].tolist(), name_map

@st.cache_resource(ttl=86400)
def load_nifty50_symbols():
    """Load Nifty 50 constituents with retry logic"""
    url = "https://archives.nseindia.com/content/indices/ind_nifty50list.csv"

    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Connection": "keep-alive"
    }

    for attempt in range(3):
        try:
            r = requests.get(url, headers=headers, timeout=10)
            r.raise_for_status()
            df = pd.read_csv(io.StringIO(r.text))
            return [s + ".NS" for s in df["Symbol"]]
        except Exception:
            time.sleep(2)

    st.error("âŒ Failed to load Nifty 50 list from NSE")
    return []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# INDICATORS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def compute_rsi(series, period=14):
    """Compute RSI with edge case handling"""
    if len(series) < period + 1:
        return None
    
    delta = series.diff()
    gain = delta.where(delta > 0, 0).rolling(period).mean()
    loss = -delta.where(delta < 0, 0).rolling(period).mean()
    
    # Handle edge cases
    if pd.isna(gain.iloc[-1]) or pd.isna(loss.iloc[-1]):
        return None
    if loss.iloc[-1] == 0:
        return 100.0
    if gain.iloc[-1] == 0:
        return 0.0
    
    rs = gain.iloc[-1] / loss.iloc[-1]
    rsi = 100 - (100 / (1 + rs))
    
    return round(rsi, 1)

def log_rs(p, p0, b, b0):
    """Log-based relative strength calculation"""
    return np.log(p / p0) - np.log(b / b0)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RS SCAN - OPTIMIZED WITH PRE-FILTERING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def rs_scan(kite, symbols, name_map, min_rs, min_liq, benchmark_mode):
    """Main RS scanning logic with pre-filtering and parallel fetch"""
    
    # Load instrument maps
    instrument_map = load_kite_instrument_map(kite)
    st.session_state.instrument_map = instrument_map
    
    # Debug: Show available index symbols
    st.sidebar.info("ğŸ” Checking index availability...")
    available_indices = list(instrument_map["indices"].keys())
    st.sidebar.text(f"Found {len(available_indices)} indices")
    
    # Fetch benchmark data FIRST (more reliable sequentially)
    with st.spinner("ğŸ“Š Fetching benchmark indices..."):
        bm_data = fetch_benchmarks_sequential(kite, instrument_map)
    
    if not bm_data:
        st.error("âŒ Failed to fetch any benchmark data. Check your connection and token.")
        st.stop()
    
    st.success(f"âœ… Fetched {len(bm_data)} benchmarks successfully")
    
    # PRE-FILTER: Use quote API to eliminate low-liquidity stocks
    with st.spinner("ğŸ” Pre-filtering universe with Quote API..."):
        filtered_symbols = prefilter_universe(kite, symbols, instrument_map, min_liq)
    
    reduction_pct = (1 - len(filtered_symbols) / len(symbols)) * 100 if len(symbols) > 0 else 0
    st.info(f"ğŸ“Š Pre-filter: {len(symbols)} â†’ {len(filtered_symbols)} stocks ({reduction_pct:.1f}% reduction)")
    
    if len(filtered_symbols) == 0:
        st.error("âŒ No stocks passed pre-filtering. Try relaxing liquidity criteria.")
        return pd.DataFrame(), None, pd.DataFrame()

    # PARALLEL FETCH: Stock data
    with st.spinner(f"ğŸ“¥ Fetching {len(filtered_symbols)} stocks in parallel..."):
        stock_data = fetch_in_batches_parallel(
            kite, 
            filtered_symbols, 
            instrument_map,
            max_workers=5
        )

    # Select best performing benchmark
    best_ret = -1e9
    selected_df = None
    selected_benchmark = None
    benchmark_rows = []
    min_required_days = max(RS_LOOKBACK_6M, 200) + 30

    for name, sym in BENCHMARK_CANDIDATES.items():
        df = bm_data.get(sym)
        if df is None or df.empty:
            st.sidebar.warning(f"Skipping {name} - no data")
            continue
        
        if len(df) < min_required_days:
            st.sidebar.warning(f"Skipping {name} - only {len(df)} days (need {min_required_days})")
            continue

        ret = df["Close"].iloc[-1] / df["Close"].iloc[-RS_LOOKBACK_6M] - 1
        benchmark_rows.append({
            "Benchmark": name,
            "Return_6M": round(ret * 100, 2),
            "Days": len(df),
            "Status": "âœ… Selected" if ret > best_ret else ""
        })

        if ret > best_ret:
            best_ret = ret
            selected_df = df
            selected_benchmark = name

    # Validate benchmark
    if selected_df is None or selected_df.empty:
        st.error("âŒ No valid benchmark found with sufficient history.")
        if benchmark_rows:
            st.dataframe(pd.DataFrame(benchmark_rows))
        return pd.DataFrame(), None, pd.DataFrame()

    st.success(f"ğŸ“Š Selected benchmark: {selected_benchmark} ({len(selected_df)} days)")

    # Scan stocks
    results = []
    filter_stats = {
        "total": len(stock_data),
        "short_history": 0,
        "below_dma200": 0,
        "low_liquidity": 0,
        "passed": 0
    }

    with st.spinner(f"ğŸ” Scanning {len(stock_data)} stocks against {selected_benchmark}..."):
        for sym, df in stock_data.items():
            if df.empty or len(df) < 250:
                filter_stats["short_history"] += 1
                continue

            close = df["Close"]
            price = close.iloc[-1]
            dma200 = close.rolling(200).mean().iloc[-1]
            liq = (close * df["Volume"]).tail(30).mean() / 1e7

            # Apply filters
            if price < dma200 * 0.95:
                filter_stats["below_dma200"] += 1
                continue
                
            if liq < min_liq:
                filter_stats["low_liquidity"] += 1
                continue

            # Calculate RS metrics
            try:
                rs6 = log_rs(price, close.iloc[-RS_LOOKBACK_6M],
                             selected_df["Close"].iloc[-1],
                             selected_df["Close"].iloc[-RS_LOOKBACK_6M])

                rs3 = log_rs(price, close.iloc[-RS_LOOKBACK_3M],
                             selected_df["Close"].iloc[-1],
                             selected_df["Close"].iloc[-RS_LOOKBACK_3M])

                rs_delta = rs3 - rs6
            except:
                continue
            
            # Calculate RSI at multiple timeframes
            rsi_d = compute_rsi(close)
            rsi_w = compute_rsi(close.resample("W-FRI").last()) if len(close) > 100 else None
            rsi_m = compute_rsi(close.resample("ME").last()) if len(close) > 400 else None

            # Generate TradingView link
            clean = sym.replace(".NS", "")
            tv = f"https://tradingview.com/chart/?symbol=NSE%3A{clean}"

            results.append({
                "Symbol": clean,
                "Name": name_map.get(clean, ""),
                "Price": round(price, 2),
                "RS": round(rs6, 3),
                "RS_3M": round(rs3, 3),
                "RS_6M": round(rs6, 3),
                "RS_Delta": round(rs_delta, 3),
                "LiquidityCr": round(liq, 1),
                "RSI_D": rsi_d,
                "RSI_W": rsi_w,
                "RSI_M": rsi_m,
                "Chart": tv
            })
            filter_stats["passed"] += 1

    # Create results DataFrame
    df = pd.DataFrame(results)
    
    if len(df) == 0:
        st.warning("âš ï¸ No stocks passed the filters.")
        with st.expander("ğŸ” Filter Statistics"):
            for key, val in filter_stats.items():
                st.write(f"**{key}:** {val}")
    
    # Calculate RS rank
    df["RS_Rank"] = df["RS_6M"].rank(pct=True) * 100
    df["Momentum"] = np.where(
        df["RS_Delta"] > 0, "ğŸš€ Improving", 
        np.where(df["RS_Delta"] < 0, "ğŸ“‰ Slowing", "â¡ï¸ Stable")
    )
    
    # Create benchmark table
    bm_table = pd.DataFrame(benchmark_rows).sort_values("Return_6M", ascending=False)

    # Display filter statistics
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ“Š Scan Summary")
    st.sidebar.metric("Stocks Scanned", filter_stats["total"])
    st.sidebar.metric("Passed Filters", filter_stats["passed"])
    st.sidebar.metric("Final Results", len(df[df["RS_Rank"] >= min_rs]))

    return (
        df[df["RS_Rank"] >= min_rs].sort_values("RS_Rank", ascending=False),
        selected_benchmark,
        bm_table
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    # Header
    st.markdown("""
    <div style='background: linear-gradient(135deg,#667eea,#764ba2,#f093fb);
                padding:1.2rem;border-radius:14px;color:white;text-align:center'>
        <h2 style='margin:0'>ğŸ† NSE RS Leaders PRO - OPTIMIZED</h2>
        <p style='margin:0;font-size:0.9rem'>Parallel processing â€¢ Pre-filtering â€¢ 10-15x faster</p>
    </div>
    """, unsafe_allow_html=True)

    # Authentication
    kite = get_kite_from_secrets()
    if not kite:
        st.error("âš ï¸ **Admin:** Update KITE_ACCESS_TOKEN in Streamlit Cloud settings")
        st.info("ğŸ’¡ **Tip:** Generate a new access token and update it in Settings â†’ Secrets")
        st.stop()

    # Sidebar configuration
    st.sidebar.markdown("### âš™ï¸ Scan Configuration")
    
    universe = st.sidebar.radio(
        "Universe",
        ["Nifty 50", "Full NSE"],
        help="Nifty 50 = ~30 sec, Full NSE = ~3-5 min (optimized)"
    )
    
    benchmark_mode = st.sidebar.radio(
        "Benchmark Selection", 
        ["Auto"],
        help="Automatically selects the best performing benchmark index"
    )
    
    min_rs = st.sidebar.slider(
        "Min RS Rank %",
        60, 95, MIN_RS_RANK,
        help="Only show stocks with RS rank above this percentile"
    )
    
    min_liq = st.sidebar.slider(
        "Min Liquidity â‚¹Cr",
        1, 20, MIN_LIQUIDITY_CR,
        help="Minimum average daily liquidity in crores"
    )

    # Scan button
    if st.sidebar.button("ğŸš€ RUN SCAN", use_container_width=True, type="primary"):
        # Load universe
        syms, name_map = load_nse_universe()
        symbols = load_nifty50_symbols() if universe == "Nifty 50" else syms

        if not symbols:
            st.error("âŒ Failed to load stock universe")
            st.stop()

        st.info(f"ğŸ“Š Loaded {len(symbols)} symbols from {universe}")

        # Run scan
        scan_start = time.time()
        df, selected_benchmark, bm_table = rs_scan(kite, symbols, name_map, min_rs, min_liq, benchmark_mode)
        scan_duration = time.time() - scan_start

        # Display results
        if len(df) > 0:
            st.success(f"âœ… Found **{len(df)} stocks** in {scan_duration:.1f}s")
            
            # Benchmark info
            st.markdown("---")
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.markdown(f"### ğŸ“Š Benchmark: **{selected_benchmark}**")
                st.caption("All RS calculations are relative to this index")
            
            with col2:
                best_perf = bm_table.iloc[0]["Return_6M"]
                st.metric("6M Return", f"{best_perf}%")

            # Benchmark comparison table
            with st.expander("ğŸ“ˆ View All Benchmark Returns"):
                st.dataframe(
                    bm_table,
                    hide_index=True,
                    use_container_width=True
                )

            st.markdown("---")
            st.markdown(f"### ğŸ¯ Top RS Leaders (Rank â‰¥ {min_rs}%)")

            # Style the dataframe
            def rsi_color(v):
                if pd.isna(v): return ""
                if v >= 60: return "background-color:#d4edda;color:#155724"
                if v <= 40: return "background-color:#f8d7da;color:#721c24"
                return ""

            styled = df.style.format({
                "Price": "â‚¹{:.2f}",
                "RS": "{:.3f}",
                "RS_6M": "{:.3f}",
                "RS_3M": "{:.3f}",
                "RS_Delta": "{:+.3f}",
                "LiquidityCr": "â‚¹{:.1f}Cr",
                "RSI_D": lambda x: f"{x:.1f}" if pd.notna(x) else "-",
                "RSI_W": lambda x: f"{x:.1f}" if pd.notna(x) else "-",
                "RSI_M": lambda x: f"{x:.1f}" if pd.notna(x) else "-",
                "RS_Rank": "{:.1f}%"
            }).background_gradient(
                subset=["RS_Rank"], 
                cmap="RdYlGn",
                vmin=min_rs,
                vmax=100
            ).map(rsi_color, subset=["RSI_D", "RSI_W", "RSI_M"])

            # Display dataframe
            st.dataframe(
                styled,
                use_container_width=True,
                hide_index=True,
                column_config={
                    "Chart": st.column_config.LinkColumn(
                        "Chart",
                        display_text="ğŸ“ˆ View"
                    ),
                    "Momentum": st.column_config.TextColumn(
                        "Momentum",
                        help="RS 3M vs 6M trend"
                    )
                },
                height=600
            )

            # Download CSV
            csv = df.to_csv(index=False).encode("utf-8")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M")
            st.download_button(
                "ğŸ“¥ Download Results as CSV",
                csv,
                f"RS_Leaders_{selected_benchmark.replace(' ', '_')}_{timestamp}.csv",
                mime="text/csv",
                use_container_width=True,
                type="primary"
            )

            # Key insights
            st.markdown("---")
            st.markdown("### ğŸ’¡ Key Insights")
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Avg RS Rank", f"{df['RS_Rank'].mean():.1f}%")
            
            with col2:
                improving = len(df[df["RS_Delta"] > 0])
                st.metric("Improving Momentum", f"{improving} stocks")
            
            with col3:
                overbought = len(df[df["RSI_D"] > 70])
                st.metric("RSI > 70 (Daily)", f"{overbought} stocks")
            
            with col4:
                avg_liq = df["LiquidityCr"].mean()
                st.metric("Avg Liquidity", f"â‚¹{avg_liq:.1f}Cr")

        else:
            st.warning("âš ï¸ No stocks passed the filters. Try adjusting the criteria.")

    # Info section
    with st.expander("â„¹ï¸ How This Works (Optimized Version)"):
        st.markdown("""
        **ğŸš€ Performance Optimizations:**
        - **Pre-filtering**: Quote API eliminates low-liquidity stocks before fetching historical data
        - **Parallel processing**: 5 concurrent workers with rate limiting (3 req/sec)
        - **Reduced data range**: 300 days instead of 730 (sufficient for all calculations)
        - **Result**: 10-15x faster scans (Full NSE: ~3-5 min vs 30-40 min)
        
        **Relative Strength (RS) Strategy:**
        1. Automatically selects the best performing benchmark index (6M return)
        2. Calculates log-based RS for each stock vs. benchmark
        3. Filters stocks trading above 200-day moving average
        4. Applies liquidity filters (min â‚¹5Cr avg daily volume)
        5. Ranks stocks by RS percentile
        6. Shows multi-timeframe RSI for confluence
        
        **RS Delta:** Difference between 3M and 6M RS (positive = accelerating)
        
        **RSI Color Code:**
        - ğŸŸ¢ Green (â‰¥60): Potentially overbought
        - ğŸ”´ Red (â‰¤40): Potentially oversold
        - âšª Neutral: Between 40-60
        
        **Best Practices:**
        - Focus on RS Rank > 80% with improving momentum
        - Avoid stocks with RSI > 70 on all timeframes (overextended)
        - Check TradingView charts before taking positions
        """)

if __name__ == "__main__":
    main()